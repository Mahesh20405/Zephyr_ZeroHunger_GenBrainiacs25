{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d84526",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-04-04T21:12:37.837560Z",
     "iopub.status.busy": "2025-04-04T21:12:37.837050Z",
     "iopub.status.idle": "2025-04-04T21:12:50.883577Z",
     "shell.execute_reply": "2025-04-04T21:12:50.880449Z"
    },
    "id": "PQhxAik1gFOG",
    "outputId": "6c8dab90-514b-4ca1-bb24-61db4f7244df",
    "papermill": {
     "duration": 13.055397,
     "end_time": "2025-04-04T21:12:50.885577",
     "exception": true,
     "start_time": "2025-04-04T21:12:37.830180",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auth\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultCredentialsError\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Food Monitoring System - Deep Q-Learning Training Script\n",
    "# This script trains a reinforcement learning model to optimize food distribution decisions\n",
    "# based on sensor data from the monitoring system\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import gspread\n",
    "import requests\n",
    "import csv\n",
    "from google.colab import auth\n",
    "from google.auth import default\n",
    "from google.auth.exceptions import DefaultCredentialsError\n",
    "from google.auth.transport.requests import Request\n",
    "from datetime import datetime\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# Authenticate user only once with the modern approach\n",
    "auth.authenticate_user()\n",
    "try:\n",
    "    # Get modern credentials\n",
    "    credentials, _ = default()\n",
    "    gc = gspread.authorize(credentials)\n",
    "except DefaultCredentialsError as e:\n",
    "    print(\"Error: Could not get valid credentials. Make sure you're authenticated properly.\")\n",
    "    raise e\n",
    "\n",
    "# Open the spreadsheet\n",
    "SPREADSHEET_URL = \"spreadsheet_url\"\n",
    "spreadsheet = gc.open_by_url(SPREADSHEET_URL)\n",
    "\n",
    "# Access the sheets (only SensorData and Recipients via gspread)\n",
    "sensor_data_sheet = spreadsheet.worksheet(\"SensorData\")\n",
    "recipients_sheet = spreadsheet.worksheet(\"Recipients\")\n",
    "\n",
    "# GAS URL for accessing EmailLog and TrainingMetrics\n",
    "GAS_URL = \"gas_url\"\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # discount factor\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "MEMORY_SIZE = 10000\n",
    "MIN_ENTRIES = 100  # Minimum entries before training\n",
    "\n",
    "# Define state space and action space\n",
    "# State: [temperature, humidity, gas_level, time_since_storage]\n",
    "# Actions: 0 = Keep in warehouse, 1 = Send to market, 2 = Send to food bank/NGO\n",
    "\n",
    "# Named tuple for storing transitions\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Replay Memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Neural Network for Deep Q Learning\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(data):\n",
    "    # Convert data to appropriate formats\n",
    "    data_df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "    # Convert numerical columns, coercing errors to NaN\n",
    "    for col in ['Temperature', 'Humidity', 'Gas']:\n",
    "        data_df[col] = pd.to_numeric(data_df[col], errors='coerce')  # Handle invalid values\n",
    "\n",
    "    # Drop rows with NaN in critical sensor columns\n",
    "    data_df.dropna(subset=['Temperature', 'Humidity', 'Gas'], inplace=True)\n",
    "\n",
    "    # Proceed only if there's data remaining\n",
    "    if data_df.empty:\n",
    "        raise ValueError(\"No valid data remaining after preprocessing.\")\n",
    "\n",
    "    # Create time feature (hours since storage)\n",
    "    data_df['Timestamp'] = pd.to_datetime(data_df['Timestamp'])\n",
    "    data_df = data_df.sort_values('Timestamp')\n",
    "    data_df['TimeSinceStart'] = (data_df['Timestamp'] - data_df['Timestamp'].min()).dt.total_seconds() / 3600\n",
    "\n",
    "    # One-hot encode status\n",
    "    status_map = {'Normal': 0, 'At Risk': 1, 'Spoiled': 2}\n",
    "    data_df['StatusCode'] = data_df['Status'].map(status_map)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Function to calculate reward based on food status and action\n",
    "def calculate_reward(status, action):\n",
    "    # Action: 0 = Keep, 1 = Market, 2 = Food Bank/NGO\n",
    "\n",
    "    if status == 'Normal':  # Normal condition\n",
    "        if action == 0:  # Keep in storage\n",
    "            return 1\n",
    "        elif action == 1:  # Send to market\n",
    "            return 0\n",
    "        else:  # Send to NGO\n",
    "            return -1\n",
    "\n",
    "    elif status == 'At Risk':  # At risk condition\n",
    "        if action == 0:  # Keep in storage\n",
    "            return -1\n",
    "        elif action == 1:  # Send to market\n",
    "            return 1\n",
    "        else:  # Send to NGO\n",
    "            return 0\n",
    "\n",
    "    else:  # Spoiled condition\n",
    "        if action == 0:  # Keep in storage\n",
    "            return -2\n",
    "        elif action == 1:  # Send to market\n",
    "            return -3\n",
    "        else:  # Send to NGO\n",
    "            return 1\n",
    "\n",
    "# Function to get action from epsilon-greedy policy\n",
    "def select_action(state, policy_net, steps_done, n_actions=3):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n",
    "\n",
    "# Function to optimize model\n",
    "def optimize_model(policy_net, target_net, optimizer, memory):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Main training function\n",
    "def train_model(data_df, cycle):\n",
    "    # Initialize models\n",
    "    input_size = 4  # [temperature, humidity, gas, time_since_storage]\n",
    "    output_size = 3  # [keep, market, NGO]\n",
    "\n",
    "    policy_net = DQN(input_size, output_size)\n",
    "    target_net = DQN(input_size, output_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.RMSprop(policy_net.parameters())\n",
    "    memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "    steps_done = 0\n",
    "    num_episodes = 50\n",
    "    metrics = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize environment\n",
    "        episode_data = data_df.sample(n=10).sort_values('TimeSinceStart')\n",
    "        current_idx = 0\n",
    "        total_reward = 0\n",
    "        losses = []\n",
    "        correct_actions = 0\n",
    "\n",
    "        state = torch.tensor([episode_data.iloc[current_idx][['Temperature', 'Humidity', 'Gas', 'TimeSinceStart']].values],\n",
    "                             dtype=torch.float32)\n",
    "\n",
    "        for t in range(len(episode_data) - 1):\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, policy_net, steps_done)\n",
    "            steps_done += 1\n",
    "\n",
    "            # Move to next state\n",
    "            current_idx += 1\n",
    "            next_state = torch.tensor([episode_data.iloc[current_idx][['Temperature', 'Humidity', 'Gas', 'TimeSinceStart']].values],\n",
    "                                      dtype=torch.float32)\n",
    "\n",
    "            # Get status and calculate reward\n",
    "            status = episode_data.iloc[current_idx]['Status']\n",
    "            reward_val = calculate_reward(status, action.item())\n",
    "            reward = torch.tensor([reward_val], dtype=torch.float32)\n",
    "\n",
    "            total_reward += reward_val\n",
    "\n",
    "            # Determine correct action based on status\n",
    "            correct_action = 0  # Keep in storage\n",
    "            if status == 'At Risk':\n",
    "                correct_action = 1  # Send to market\n",
    "            elif status == 'Spoiled':\n",
    "                correct_action = 2  # Send to NGO\n",
    "\n",
    "            if action.item() == correct_action:\n",
    "                correct_actions += 1\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization\n",
    "            loss = optimize_model(policy_net, target_net, optimizer, memory)\n",
    "            if loss > 0:\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Update the target network\n",
    "            if t % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Calculate episode metrics\n",
    "        avg_loss = np.mean(losses) if losses else 0\n",
    "        accuracy = correct_actions / (len(episode_data) - 1) * 100\n",
    "\n",
    "        print(f\"Episode {i_episode+1}/{num_episodes} - \"\n",
    "              f\"Loss: {avg_loss:.4f}, Reward: {total_reward:.2f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Save metrics\n",
    "        metrics.append({\n",
    "            'epoch': i_episode + 1,\n",
    "            'loss': avg_loss,\n",
    "            'reward': total_reward,\n",
    "            'accuracy': accuracy,\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "    # Save model\n",
    "    torch.save(policy_net.state_dict(), f'food_monitoring_model_cycle_{cycle}.pth')\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Function to save metrics via GAS\n",
    "def save_metrics_to_sheet(metrics, cycle):\n",
    "    # Save locally as CSV\n",
    "    df = pd.DataFrame(metrics)\n",
    "    csv_filename = f'training_metrics_{cycle}.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Read CSV data\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_data = f.read()\n",
    "\n",
    "    # Prepare payload for GAS\n",
    "    payload = {\n",
    "        'action': 'append',\n",
    "        'sheetName': 'TrainingMetrics',\n",
    "        'csvData': csv_data\n",
    "    }\n",
    "\n",
    "    # Send POST request to GAS\n",
    "    try:\n",
    "        response = requests.post(GAS_URL, data=payload)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Training metrics for cycle {cycle} sent to Google Sheets via GAS.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to send metrics: {e}\")\n",
    "\n",
    "# Main execution loop\n",
    "def main():\n",
    "    last_entry_count = 0\n",
    "    current_cycle = 0\n",
    "\n",
    "    # Check existing TrainingMetrics via GAS\n",
    "    try:\n",
    "        params = {'action': 'get', 'sheetName': 'TrainingMetrics'}\n",
    "        response = requests.get(GAS_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        csv_data = response.text\n",
    "        existing_metrics = list(csv.reader(csv_data.splitlines()))\n",
    "        if len(existing_metrics) > 1:\n",
    "            current_cycle = max(int(row[0]) for row in existing_metrics[1:])\n",
    "        else:\n",
    "            current_cycle = 0\n",
    "        print(f\"Found existing cycles up to {current_cycle}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching existing metrics: {e}\")\n",
    "        current_cycle = 0\n",
    "\n",
    "    print(f\"Starting monitoring for training. Current cycle: {current_cycle}\")\n",
    "    print(f\"Will begin training when sensor data reaches {MIN_ENTRIES} entries.\")\n",
    "\n",
    "    while True:\n",
    "        # Get current sensor data\n",
    "        sensor_data = sensor_data_sheet.get_all_values()\n",
    "        current_entry_count = len(sensor_data) - 1  # Subtract header row\n",
    "\n",
    "        print(f\"Current entries: {current_entry_count}, Last checked: {last_entry_count}\")\n",
    "\n",
    "        # Check if we've collected enough new data for training\n",
    "        if current_entry_count >= last_entry_count + MIN_ENTRIES:\n",
    "            current_cycle += 1\n",
    "            print(f\"Starting training cycle {current_cycle}\")\n",
    "\n",
    "            # Preprocess data\n",
    "            data_df = preprocess_data(sensor_data)\n",
    "\n",
    "            # Train model\n",
    "            metrics = train_model(data_df, current_cycle)\n",
    "\n",
    "            # Save metrics\n",
    "            save_metrics_to_sheet(metrics, current_cycle)\n",
    "\n",
    "            # Update last entry count\n",
    "            last_entry_count = current_entry_count\n",
    "\n",
    "            print(f\"Completed training cycle {current_cycle}\")\n",
    "            print(f\"Waiting for {MIN_ENTRIES} more entries before next cycle\")\n",
    "\n",
    "        # Wait before checking again\n",
    "        time.sleep(60)  # Check every minute\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": true,
   "input_path": "E:\\Zephyr\\Software Configuration\\DeepQ_Reinforcement.ipynb",
   "output_path": "output_notebook.ipynb",
   "parameters": {},
   "start_time": "2025-04-04T21:12:35.844083",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
