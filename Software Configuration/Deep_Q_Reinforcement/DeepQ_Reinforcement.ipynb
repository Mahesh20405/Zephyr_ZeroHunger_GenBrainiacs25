{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PQhxAik1gFOG",
        "outputId": "6c8dab90-514b-4ca1-bb24-61db4f7244df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing cycles up to 0\n",
            "Starting monitoring for training. Current cycle: 0\n",
            "Will begin training when sensor data reaches 100 entries.\n",
            "Current entries: 42, Last checked: 0\n",
            "Current entries: 48, Last checked: 0\n",
            "Current entries: 54, Last checked: 0\n",
            "Current entries: 60, Last checked: 0\n",
            "Current entries: 66, Last checked: 0\n",
            "Current entries: 72, Last checked: 0\n",
            "Current entries: 78, Last checked: 0\n",
            "Current entries: 84, Last checked: 0\n",
            "Current entries: 90, Last checked: 0\n",
            "Current entries: 96, Last checked: 0\n",
            "Current entries: 101, Last checked: 0\n",
            "Starting training cycle 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-c8643fe797ee>:212: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  state = torch.tensor([episode_data.iloc[current_idx][['Temperature', 'Humidity', 'Gas', 'TimeSinceStart']].values],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/50 - Loss: 0.0000, Reward: -8.00, Accuracy: 11.11%\n",
            "Episode 2/50 - Loss: 0.0000, Reward: -4.00, Accuracy: 44.44%\n",
            "Episode 3/50 - Loss: 0.0000, Reward: -2.00, Accuracy: 44.44%\n",
            "Episode 4/50 - Loss: 0.0000, Reward: 1.00, Accuracy: 55.56%\n",
            "Episode 5/50 - Loss: 0.0000, Reward: -5.00, Accuracy: 44.44%\n",
            "Episode 6/50 - Loss: 0.0000, Reward: -1.00, Accuracy: 44.44%\n",
            "Episode 7/50 - Loss: 0.0000, Reward: -3.00, Accuracy: 44.44%\n",
            "Episode 8/50 - Loss: 666.9000, Reward: -1.00, Accuracy: 33.33%\n",
            "Episode 9/50 - Loss: 431.2752, Reward: -1.00, Accuracy: 33.33%\n",
            "Episode 10/50 - Loss: 225.8450, Reward: 1.00, Accuracy: 44.44%\n",
            "Episode 11/50 - Loss: 130.9399, Reward: -4.00, Accuracy: 44.44%\n",
            "Episode 12/50 - Loss: 158.4396, Reward: -10.00, Accuracy: 0.00%\n",
            "Episode 13/50 - Loss: 187.0251, Reward: -10.00, Accuracy: 22.22%\n",
            "Episode 14/50 - Loss: 130.6040, Reward: -6.00, Accuracy: 22.22%\n",
            "Episode 15/50 - Loss: 157.5330, Reward: -9.00, Accuracy: 11.11%\n",
            "Episode 16/50 - Loss: 193.5501, Reward: 1.00, Accuracy: 33.33%\n",
            "Episode 17/50 - Loss: 214.6410, Reward: -5.00, Accuracy: 11.11%\n",
            "Episode 18/50 - Loss: 187.7152, Reward: 1.00, Accuracy: 44.44%\n",
            "Episode 19/50 - Loss: 199.2359, Reward: -6.00, Accuracy: 22.22%\n",
            "Episode 20/50 - Loss: 189.0232, Reward: -8.00, Accuracy: 33.33%\n",
            "Episode 21/50 - Loss: 195.6948, Reward: -6.00, Accuracy: 44.44%\n",
            "Episode 22/50 - Loss: 194.2052, Reward: 5.00, Accuracy: 66.67%\n",
            "Episode 23/50 - Loss: 212.2909, Reward: -4.00, Accuracy: 22.22%\n",
            "Episode 24/50 - Loss: 194.6238, Reward: -6.00, Accuracy: 22.22%\n",
            "Episode 25/50 - Loss: 206.9745, Reward: -7.00, Accuracy: 44.44%\n",
            "Episode 26/50 - Loss: 227.7247, Reward: -10.00, Accuracy: 11.11%\n",
            "Episode 27/50 - Loss: 224.2999, Reward: -1.00, Accuracy: 55.56%\n",
            "Episode 28/50 - Loss: 215.8839, Reward: 6.00, Accuracy: 77.78%\n",
            "Episode 29/50 - Loss: 228.6147, Reward: -6.00, Accuracy: 11.11%\n",
            "Episode 30/50 - Loss: 254.3078, Reward: -5.00, Accuracy: 11.11%\n",
            "Episode 31/50 - Loss: 212.3396, Reward: -1.00, Accuracy: 44.44%\n",
            "Episode 32/50 - Loss: 214.7447, Reward: -3.00, Accuracy: 44.44%\n",
            "Episode 33/50 - Loss: 210.8915, Reward: -11.00, Accuracy: 0.00%\n",
            "Episode 34/50 - Loss: 239.1325, Reward: -12.00, Accuracy: 11.11%\n",
            "Episode 35/50 - Loss: 238.2502, Reward: -4.00, Accuracy: 55.56%\n",
            "Episode 36/50 - Loss: 239.6355, Reward: -4.00, Accuracy: 44.44%\n",
            "Episode 37/50 - Loss: 247.6845, Reward: -9.00, Accuracy: 0.00%\n",
            "Episode 38/50 - Loss: 244.9547, Reward: -11.00, Accuracy: 0.00%\n",
            "Episode 39/50 - Loss: 257.7564, Reward: -6.00, Accuracy: 33.33%\n",
            "Episode 40/50 - Loss: 277.7600, Reward: -7.00, Accuracy: 33.33%\n",
            "Episode 41/50 - Loss: 265.8814, Reward: -5.00, Accuracy: 33.33%\n",
            "Episode 42/50 - Loss: 284.8441, Reward: -2.00, Accuracy: 55.56%\n",
            "Episode 43/50 - Loss: 254.5691, Reward: 4.00, Accuracy: 55.56%\n",
            "Episode 44/50 - Loss: 303.7675, Reward: -4.00, Accuracy: 55.56%\n",
            "Episode 45/50 - Loss: 308.9327, Reward: -4.00, Accuracy: 11.11%\n",
            "Episode 46/50 - Loss: 288.7437, Reward: -1.00, Accuracy: 11.11%\n",
            "Episode 47/50 - Loss: 304.1677, Reward: -7.00, Accuracy: 11.11%\n",
            "Episode 48/50 - Loss: 312.8851, Reward: -6.00, Accuracy: 44.44%\n",
            "Episode 49/50 - Loss: 320.4454, Reward: -4.00, Accuracy: 33.33%\n",
            "Episode 50/50 - Loss: 338.4145, Reward: -4.00, Accuracy: 44.44%\n",
            "Training metrics for cycle 1 sent to Google Sheets via GAS.\n",
            "Completed training cycle 1\n",
            "Waiting for 100 more entries before next cycle\n",
            "Current entries: 108, Last checked: 101\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c8643fe797ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-c8643fe797ee>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;31m# Wait before checking again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check every minute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Food Monitoring System - Deep Q-Learning Training Script\n",
        "# This script trains a reinforcement learning model to optimize food distribution decisions\n",
        "# based on sensor data from the monitoring system\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import time\n",
        "import gspread\n",
        "import requests\n",
        "import csv\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from google.auth.exceptions import DefaultCredentialsError\n",
        "from google.auth.transport.requests import Request\n",
        "from datetime import datetime\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Authenticate user only once with the modern approach\n",
        "auth.authenticate_user()\n",
        "try:\n",
        "    # Get modern credentials\n",
        "    credentials, _ = default()\n",
        "    gc = gspread.authorize(credentials)\n",
        "except DefaultCredentialsError as e:\n",
        "    print(\"Error: Could not get valid credentials. Make sure you're authenticated properly.\")\n",
        "    raise e\n",
        "\n",
        "# Open the spreadsheet\n",
        "SPREADSHEET_URL = \"google_sheet_link\"\n",
        "spreadsheet = gc.open_by_url(SPREADSHEET_URL)\n",
        "\n",
        "# Access the sheets (only SensorData and Recipients via gspread)\n",
        "sensor_data_sheet = spreadsheet.worksheet(\"SensorData\")\n",
        "recipients_sheet = spreadsheet.worksheet(\"Recipients\")\n",
        "\n",
        "# GAS URL for accessing EmailLog and TrainingMetrics\n",
        "GAS_URL = \"GAS_URL\"\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99  # discount factor\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "MEMORY_SIZE = 10000\n",
        "MIN_ENTRIES = 100  # Minimum entries before training\n",
        "\n",
        "# Define state space and action space\n",
        "# State: [temperature, humidity, gas_level, time_since_storage]\n",
        "# Actions: 0 = Keep in warehouse, 1 = Send to market, 2 = Send to food bank/NGO\n",
        "\n",
        "# Named tuple for storing transitions\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# Replay Memory\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Neural Network for Deep Q Learning\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Function to preprocess data\n",
        "def preprocess_data(data):\n",
        "    # Convert data to appropriate formats\n",
        "    data_df = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "    # Convert numerical columns, coercing errors to NaN\n",
        "    for col in ['Temperature', 'Humidity', 'Gas']:\n",
        "        data_df[col] = pd.to_numeric(data_df[col], errors='coerce')  # Handle invalid values\n",
        "\n",
        "    # Drop rows with NaN in critical sensor columns\n",
        "    data_df.dropna(subset=['Temperature', 'Humidity', 'Gas'], inplace=True)\n",
        "\n",
        "    # Proceed only if there's data remaining\n",
        "    if data_df.empty:\n",
        "        raise ValueError(\"No valid data remaining after preprocessing.\")\n",
        "\n",
        "    # Create time feature (hours since storage)\n",
        "    data_df['Timestamp'] = pd.to_datetime(data_df['Timestamp'])\n",
        "    data_df = data_df.sort_values('Timestamp')\n",
        "    data_df['TimeSinceStart'] = (data_df['Timestamp'] - data_df['Timestamp'].min()).dt.total_seconds() / 3600\n",
        "\n",
        "    # One-hot encode status\n",
        "    status_map = {'Normal': 0, 'At Risk': 1, 'Spoiled': 2}\n",
        "    data_df['StatusCode'] = data_df['Status'].map(status_map)\n",
        "\n",
        "    return data_df\n",
        "\n",
        "# Function to calculate reward based on food status and action\n",
        "def calculate_reward(status, action):\n",
        "    # Action: 0 = Keep, 1 = Market, 2 = Food Bank/NGO\n",
        "\n",
        "    if status == 'Normal':  # Normal condition\n",
        "        if action == 0:  # Keep in storage\n",
        "            return 1\n",
        "        elif action == 1:  # Send to market\n",
        "            return 0\n",
        "        else:  # Send to NGO\n",
        "            return -1\n",
        "\n",
        "    elif status == 'At Risk':  # At risk condition\n",
        "        if action == 0:  # Keep in storage\n",
        "            return -1\n",
        "        elif action == 1:  # Send to market\n",
        "            return 1\n",
        "        else:  # Send to NGO\n",
        "            return 0\n",
        "\n",
        "    else:  # Spoiled condition\n",
        "        if action == 0:  # Keep in storage\n",
        "            return -2\n",
        "        elif action == 1:  # Send to market\n",
        "            return -3\n",
        "        else:  # Send to NGO\n",
        "            return 1\n",
        "\n",
        "# Function to get action from epsilon-greedy policy\n",
        "def select_action(state, policy_net, steps_done, n_actions=3):\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
        "\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n",
        "\n",
        "# Function to optimize model\n",
        "def optimize_model(policy_net, target_net, optimizer, memory):\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return 0\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Main training function\n",
        "def train_model(data_df, cycle):\n",
        "    # Initialize models\n",
        "    input_size = 4  # [temperature, humidity, gas, time_since_storage]\n",
        "    output_size = 3  # [keep, market, NGO]\n",
        "\n",
        "    policy_net = DQN(input_size, output_size)\n",
        "    target_net = DQN(input_size, output_size)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.RMSprop(policy_net.parameters())\n",
        "    memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "    steps_done = 0\n",
        "    num_episodes = 50\n",
        "    metrics = []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        # Initialize environment\n",
        "        episode_data = data_df.sample(n=10).sort_values('TimeSinceStart')\n",
        "        current_idx = 0\n",
        "        total_reward = 0\n",
        "        losses = []\n",
        "        correct_actions = 0\n",
        "\n",
        "        state = torch.tensor([episode_data.iloc[current_idx][['Temperature', 'Humidity', 'Gas', 'TimeSinceStart']].values],\n",
        "                             dtype=torch.float32)\n",
        "\n",
        "        for t in range(len(episode_data) - 1):\n",
        "            # Select and perform an action\n",
        "            action = select_action(state, policy_net, steps_done)\n",
        "            steps_done += 1\n",
        "\n",
        "            # Move to next state\n",
        "            current_idx += 1\n",
        "            next_state = torch.tensor([episode_data.iloc[current_idx][['Temperature', 'Humidity', 'Gas', 'TimeSinceStart']].values],\n",
        "                                      dtype=torch.float32)\n",
        "\n",
        "            # Get status and calculate reward\n",
        "            status = episode_data.iloc[current_idx]['Status']\n",
        "            reward_val = calculate_reward(status, action.item())\n",
        "            reward = torch.tensor([reward_val], dtype=torch.float32)\n",
        "\n",
        "            total_reward += reward_val\n",
        "\n",
        "            # Determine correct action based on status\n",
        "            correct_action = 0  # Keep in storage\n",
        "            if status == 'At Risk':\n",
        "                correct_action = 1  # Send to market\n",
        "            elif status == 'Spoiled':\n",
        "                correct_action = 2  # Send to NGO\n",
        "\n",
        "            if action.item() == correct_action:\n",
        "                correct_actions += 1\n",
        "\n",
        "            # Store the transition in memory\n",
        "            memory.push(state, action, next_state, reward)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            # Perform one step of the optimization\n",
        "            loss = optimize_model(policy_net, target_net, optimizer, memory)\n",
        "            if loss > 0:\n",
        "                losses.append(loss)\n",
        "\n",
        "            # Update the target network\n",
        "            if t % TARGET_UPDATE == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        # Calculate episode metrics\n",
        "        avg_loss = np.mean(losses) if losses else 0\n",
        "        accuracy = correct_actions / (len(episode_data) - 1) * 100\n",
        "\n",
        "        print(f\"Episode {i_episode+1}/{num_episodes} - \"\n",
        "              f\"Loss: {avg_loss:.4f}, Reward: {total_reward:.2f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        # Save metrics\n",
        "        metrics.append({\n",
        "            'epoch': i_episode + 1,\n",
        "            'loss': avg_loss,\n",
        "            'reward': total_reward,\n",
        "            'accuracy': accuracy,\n",
        "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        })\n",
        "\n",
        "    # Save model\n",
        "    torch.save(policy_net.state_dict(), f'food_monitoring_model_cycle_{cycle}.pth')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Function to save metrics via GAS\n",
        "def save_metrics_to_sheet(metrics, cycle):\n",
        "    # Save locally as CSV\n",
        "    df = pd.DataFrame(metrics)\n",
        "    csv_filename = f'training_metrics_{cycle}.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    # Read CSV data\n",
        "    with open(csv_filename, 'r') as f:\n",
        "        csv_data = f.read()\n",
        "\n",
        "    # Prepare payload for GAS\n",
        "    payload = {\n",
        "        'action': 'append',\n",
        "        'sheetName': 'TrainingMetrics',\n",
        "        'csvData': csv_data\n",
        "    }\n",
        "\n",
        "    # Send POST request to GAS\n",
        "    try:\n",
        "        response = requests.post(GAS_URL, data=payload)\n",
        "        response.raise_for_status()\n",
        "        print(f\"Training metrics for cycle {cycle} sent to Google Sheets via GAS.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to send metrics: {e}\")\n",
        "\n",
        "# Main execution loop\n",
        "def main():\n",
        "    last_entry_count = 0\n",
        "    current_cycle = 0\n",
        "\n",
        "    # Check existing TrainingMetrics via GAS\n",
        "    try:\n",
        "        params = {'action': 'get', 'sheetName': 'TrainingMetrics'}\n",
        "        response = requests.get(GAS_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        csv_data = response.text\n",
        "        existing_metrics = list(csv.reader(csv_data.splitlines()))\n",
        "        if len(existing_metrics) > 1:\n",
        "            current_cycle = max(int(row[0]) for row in existing_metrics[1:])\n",
        "        else:\n",
        "            current_cycle = 0\n",
        "        print(f\"Found existing cycles up to {current_cycle}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching existing metrics: {e}\")\n",
        "        current_cycle = 0\n",
        "\n",
        "    print(f\"Starting monitoring for training. Current cycle: {current_cycle}\")\n",
        "    print(f\"Will begin training when sensor data reaches {MIN_ENTRIES} entries.\")\n",
        "\n",
        "    while True:\n",
        "        # Get current sensor data\n",
        "        sensor_data = sensor_data_sheet.get_all_values()\n",
        "        current_entry_count = len(sensor_data) - 1  # Subtract header row\n",
        "\n",
        "        print(f\"Current entries: {current_entry_count}, Last checked: {last_entry_count}\")\n",
        "\n",
        "        # Check if we've collected enough new data for training\n",
        "        if current_entry_count >= last_entry_count + MIN_ENTRIES:\n",
        "            current_cycle += 1\n",
        "            print(f\"Starting training cycle {current_cycle}\")\n",
        "\n",
        "            # Preprocess data\n",
        "            data_df = preprocess_data(sensor_data)\n",
        "\n",
        "            # Train model\n",
        "            metrics = train_model(data_df, current_cycle)\n",
        "\n",
        "            # Save metrics\n",
        "            save_metrics_to_sheet(metrics, current_cycle)\n",
        "\n",
        "            # Update last entry count\n",
        "            last_entry_count = current_entry_count\n",
        "\n",
        "            print(f\"Completed training cycle {current_cycle}\")\n",
        "            print(f\"Waiting for {MIN_ENTRIES} more entries before next cycle\")\n",
        "\n",
        "        # Wait before checking again\n",
        "        time.sleep(60)  # Check every minute\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
